from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
import operator
from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.callbacks import CallbackManager
from langchain_core.callbacks.manager import CallbackManagerForToolRun
from langchain_core.callbacks.manager import AsyncCallbackManagerForToolRun

# Define state type
import os
import operator
from typing import TypedDict, Annotated
from langgraph.graph import StateGraph, END
from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document

# === Shared Types ===
class WriterState(TypedDict):
    messages: Annotated[list[AnyMessage], operator.add]
    summary: str
    longterm_memory: str
    rag_context: str
    next_scene: str

# === Mocks for RAG ===
class MockRetriever:
    def get_relevant_documents(self, query):
        return [Document(page_content="ä»–èƒŒå›äº†ä½ ï¼Œæ˜¯å‘½è¿ï¼Œä¹Ÿæ˜¯è¯•ç‚¼ã€‚")]

class MockRAGStore:
    def add_texts(self, texts):
        print("ğŸ” æ·»åŠ åˆ°RAGè®°å¿†:", texts)

# === LongBookWriter OOP Class ===
class LongBookWriter:
    def __init__(self, rules: str, longterm_path: str = "data/longterm_doc.md"):
        self.model = ChatOpenAI(model="gpt-4o", temperature=0.3)
        self.retriever = MockRetriever()
        self.rag_store = MockRAGStore()
        self.rules = SystemMessage(content=rules)
        self.longterm_path = longterm_path
        self.graph = self._build_graph()

    def _build_graph(self):
        builder = StateGraph(WriterState)
        builder.add_node("retrieve_prompt", self.retrieve_prompt)
        builder.add_node("load_longterm_memory", self.load_longterm_memory)
        builder.add_node("retrieve_rag_context", self.retrieve_rag_context)
        builder.add_node("use_shortterm_summary", self.use_shortterm_summary)
        builder.add_node("chain_of_thought", self.chain_of_thought_planner)
        builder.add_node("generate_next_scene", self.generate_next_scene)
        builder.add_node("update_memories", self.update_memories)

        builder.set_entry_point("retrieve_prompt")
        builder.add_edge("retrieve_prompt", "load_longterm_memory")
        builder.add_edge("load_longterm_memory", "retrieve_rag_context")
        builder.add_edge("retrieve_rag_context", "use_shortterm_summary")
        builder.add_edge("use_shortterm_summary", "chain_of_thought")
        builder.add_edge("chain_of_thought", "generate_next_scene")
        builder.add_edge("generate_next_scene", "update_memories")
        builder.add_edge("update_memories", END)
        return builder.compile()

    # === Graph Nodes ===
    def retrieve_prompt(self, state: WriterState):
        return {"messages": [self.rules] + state["messages"]}

    def load_longterm_memory(self, state: WriterState):
        with open(self.longterm_path, encoding="utf-8") as f:
            return {"longterm_memory": f.read()}

    def retrieve_rag_context(self, state: WriterState):
        query = state["messages"][-1].content
        docs = self.retriever.get_relevant_documents(query)
        return {"rag_context": "\n\n".join(d.page_content for d in docs)}

    def use_shortterm_summary(self, state: WriterState):
        return {"summary": state["summary"]}

    def chain_of_thought_planner(self, state: WriterState):
        prompt = f"""å‰§æƒ…æ‘˜è¦ï¼š{state['summary']}
äººç‰©è®¾å®šï¼š{state['longterm_memory']}
é£æ ¼ç‰‡æ®µï¼š{state['rag_context']}

è¯·æ€è€ƒä¸‹ä¸€ç« çš„å‘å±•ï¼ˆäº‹ä»¶ã€åŠ¨æœºã€æƒ…ç»ªï¼‰ã€‚"""
        result = self.model.invoke([HumanMessage(content=prompt)])
        return {"messages": [result]}

    def generate_next_scene(self, state: WriterState):
        prompt = f"""ä½ æ˜¯ä¸€ä½ç½‘ç»œå°è¯´ä½œå®¶ï¼Œè¯·ç»§ç»­å†™ä¸‹ä¸€ç« èŠ‚ï¼ˆçº¦1000å­—ï¼‰ã€‚
äººç‰©è®¾å®šï¼š{state['longterm_memory']}
æ‘˜è¦ï¼š{state['summary']}
é£æ ¼ç‰‡æ®µï¼š{state['rag_context']}
æ„æ€æç¤ºï¼š{state['messages'][-1].content}

è¯·ç›´æ¥è¾“å‡ºæ­£æ–‡ï¼Œä¸è¦è§£é‡Šã€‚"""
        result = self.model.invoke([HumanMessage(content=prompt)])
        return {"next_scene": result.content, "messages": [result]}

    def update_memories(self, state: WriterState):
        with open("data/generated_novel.txt", "a", encoding="utf-8") as f:
            f.write(state["next_scene"] + "\n\n")

        summary_prompt = f"è¯·æ€»ç»“ä»¥ä¸‹å°è¯´æ®µè½ä¸ºä¸‰å¥è¯ï¼š\n{state['next_scene']}"
        summary = self.model.invoke([HumanMessage(content=summary_prompt)]).content
        self.rag_store.add_texts([state["next_scene"]])
        return {"summary": summary}

    # === Run Function ===
    def run(self, user_prompt: str, summary: str = ""):
        state = {
            "messages": [HumanMessage(content=user_prompt)],
            "summary": summary,
        }
        result = self.graph.invoke(state)
        print("\nğŸ“˜ æ–°ç« èŠ‚ç”Ÿæˆ:\n")
        print(result["next_scene"])
        return result["next_scene"]
rules_cn = """
è¯·æŒ‰ä»¥ä¸‹è§„åˆ™ç”Ÿæˆå°è¯´ï¼š

1. è¯»å–å¹¶ä½¿ç”¨é•¿æœŸè®°å¿†ï¼ˆäººç‰©è®¾å®šã€èƒŒæ™¯ï¼‰ï¼›
2. ä½¿ç”¨çŸ­æœŸæ‘˜è¦å¸®åŠ©è¿è´¯å‰§æƒ…ï¼›
3. ä½¿ç”¨é£æ ¼ç‰‡æ®µæ¨¡ä»¿è¯­è¨€ã€èŠ‚å¥ï¼›
4. æ€è€ƒåç»­å‘å±•ï¼ˆäº‹ä»¶ã€æƒ…ç»ªã€å†²çªï¼‰ï¼›
5. ç”Ÿæˆæ­£æ–‡çº¦1000å­—ï¼Œç›´æ¥å†™æ•…äº‹ï¼›
6. å¦‚æœ‰æ–°è§’è‰²æˆ–é‡å¤§å‰§æƒ…è½¬æŠ˜ï¼Œè¯·åœ¨æœ«å°¾å»ºè®®æ›´æ–°é•¿æœŸè®°å¿†ã€‚
"""

writer = LongBookWriter(rules=rules_cn)

writer.run(
    user_prompt="è¯·ç»§ç»­å†™æ¥ä¸‹æ¥çš„å‰§æƒ…ï¼Œæ—çƒ¬åˆšåˆšè¸å…¥æš®éœå±±çš„è¾¹ç•Œã€‚",
    summary="æ—çƒ¬è¢«æ²ˆç„±èƒŒå›åï¼Œåªèº«é€ƒå¾€æš®éœå±±ï¼Œé€”ä¸­ç–‘äº‘é‡é‡ã€‚"
)